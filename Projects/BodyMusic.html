<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Stompbox Development Status</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1, h2 {
            color: #333;
        }
        h1 {
            text-align: center;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h2 {
            margin-top: 2em; /* Adds more space above h2 elements */
        }
        ul {
            list-style-type: disc; /* Use bullets for lists */
            padding-left: 20px; /* Indent the list items */
        }
        .home-button {
    display: inline-block;
    padding: 10px 20px;
    background-color: #333; /* Dark gray background */
    color: #fff; /* White text */
    text-decoration: none; /* Removes underline */
    font-weight: bold; /* Bold text */
    border: none; /* Removes border */
    border-radius: 5px; /* Rounded corners */
    margin-top: 20px; /* Adds some space above the button */
}

.home-button:hover {
    background-color: #444; /* Darker gray on hover */
    cursor: pointer; /* Changes cursor to pointer on hover */
}

    </style>
</head>


<body>
    <h1>Kinect-Sound Interaction Project</h1>
    <p>This project, initiated around 2009, explores the intersection of human movement and sound, utilizing the Microsoft Kinect sensor to create a real-time interactive musical/artistic system.</p>

    <div class="section">
        <h2>Project Overview</h2>
        <p>The core idea is to translate the gestures and postures of a user (likely a musician or performer) captured by the Kinect into dynamic variations of sound parameters such as frequency (pitch), volume, envelope, and timbre.</p>

        <h3>Key Project Elements:</h3>
        <ul>
            <li class="key-point"><strong>Input:</strong> 3D motion data captured by a Kinect sensor, tracking the position of key body joints (head, pelvis, shoulders, elbows, wrists).</li>
            <li class="key-point"><strong>Processing:</strong> A software system ("program") containing an "engine" and various "modules" processes the motion data.
                <ul>
                    <li><strong>Normalization (Axis Module):</strong> Normalizes 3D coordinates relative to an initial reference pose (R0), primarily using the ratio between head and pelvis positions.</li>
                    <li><strong>Dynamic Parameter Calculation:</strong> Calculates input parameters for the sound engine based on the ratio of the current pose (Rt) to the reference pose (R0).</li>
                    <li><strong>Sound Generation & Manipulation (Engine & Modules):</strong> An audio "engine" (implementation details not fully specified) generates or reproduces sounds, currently tested with 440Hz sine wave WAV files. Various modules (e.g., "dynamics," "envelope," "tone") modulate sound characteristics based on motion parameters.</li>
                    <li><strong>Volume Control (Dynamics Module):</strong> Responsible for modulating volume based on motion variations, although the specific logic is under investigation.</li>
                    <li><strong>Tone Control (Tone Module):</strong> The initial tone is calculated from the static pose (R0) and can be dynamically modified based on movements (Rt).</li>
                    <li><strong>Envelope Control (Envelope Module):</strong> Manages the sound's envelope (attack, sustain, decay) with options for manipulation.</li>
                    <li><strong>Timbre Selection (Sonority Module):</strong> Allows loading different audio sample sets to achieve timbres characteristic of various geographical regions.</li>
                </ul>
            </li>
            <li class="key-point"><strong>Output:</strong> Sounds generated and manipulated in real-time, presumably played through speakers or headphones.</li>
            <li class="key-point"><strong>User Interface (Implied):</strong> An interface with a display showing "elementary symbols" associated with different parameter manipulation operations is planned or partially implemented.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Current Status and Challenges (Debugging Phase)</h2>
        <p>The project is currently in the debugging phase, with several challenges identified:</p>
        <ul>
            <li class="key-point"><span class="problem">Initial Sound Generation:</span> The system initially produced no sound, which was resolved by identifying an issue with sending inaudible frequencies.</li>
            <li class="key-point"><span class="problem">Dynamic Sound Modification:</span> Activating dynamic frequency and volume modification results in a confusing and uncontrolled sound output.</li>
            <li class="key-point"><span class="problem">Volume Control (Dynamics Module):</span>
                <ul>
                    <li><span class="problem">Calibration:</span> The dynamics of the volume require calibration.</li>
                    <li><span class="problem">Control Logic:</span> There are concerns about the volume control logic, which appears to be based on the rate of change of the relative depth (Z-axis relative acceleration between head and pelvis) rather than a straightforward body acceleration. This deviates from the expected direct link between forward motion and sound "attack."</li>
                </ul>
            </li>
            <li class="key-point"><span class="problem">Module Integration:</span> The proper interaction between different modules (axis, dynamics, tone, envelope) needs further verification.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Recent Developments</h2>
        <ul>
            <li class="key-point"><span class="update">Envelope Parameters:</span> Addition of parameters related to the sound envelope.</li>
            <li class="key-point"><span class="update">Module Modifications:</span> Partial modifications to the "axis," "arms," and "legs" modules.</li>
            <li class="key-point"><span class="update">Dynamics Module Implementation:</span> Implementation of the "dynamics" module (internal structure details available in separate images).</li>
            <li class="key-point"><span class="update">Parameter Manipulation Definition:</span> Precise definition of parameter manipulation operations (scaling, proportion modification, space partitioning, tone, sensitivity, envelope, sonority) and their association with user interface symbols.</li>
            <li class="key-point"><span class="update">Initial Tone Calculation:</span> Definition of the algorithm for calculating the initial tone based on the static pose (R0).</li>
        </ul>
    </div>

    <div class="section">
        <h2>Next Steps and Considerations</h2>
        <ul>
            <li class="key-point">In-depth analysis of the "dynamics" module code to understand the volume control logic and address concerns about its correlation with movements.</li>
            <li class="key-point">Investigating the causes of the confusing sound output during dynamic parameter modification. This may involve reviewing the mapping of motion parameters to sound parameters or the implementation of the sound engine.</li>
            <li class="key-point">Conducting more systematic tests with different movement types and sound parameters to evaluate the system's response and identify areas for improvement.</li>
            <li class="key-point">Designing and implementing the user interface with the defined symbols to allow user interaction with the system parameters.</li>
            <li class="key-point">Considering the addition of visual feedback for the user related to captured movements and generated sound parameters.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Conclusion</h2>
        <p>The Kinect-Sound Interaction Project, while having been in development for a significant period, still faces notable challenges in its implementation, particularly regarding dynamic sound generation and volume control logic. The recent documentation of parameters and active debugging phase indicate ongoing efforts to complete the project. A thorough understanding of the "dynamics" module and the resolution of sound stability issues will be crucial for the system's success.</p>
    </div>
</body>

</html>
