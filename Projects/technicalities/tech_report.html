<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sounding Canvas</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f9;
            color: #333;
        }
        h1, h2 {
            color: #0056b3;
        }
        h1 {
            text-align: center;
        }
        pre {
            background-color: #eaeaea;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .section {
            margin-bottom: 30px;
        }
        img {
            max-width: 100%; /* Ensures image doesn't overflow its container */
            height: auto;    /* Maintains aspect ratio */
            display: block;  /* Removes extra space below images */
            margin: 0 auto;  /* Centers images if they are smaller than 100% width */
        }

        /* Optional: Add a class for specific image styling if needed */
        .full-width-image {
            width: 100%;
            height: auto;
        }

        /* Optional: Basic image gallery/grid if you have multiple images */
        .image-gallery {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); /* Adjust minmax as needed */
            gap: 20px; /* Space between images */
            margin-top: 20px;
        }
        .image-gallery img {
            width: 100%;
            height: auto;
            object-fit: cover; /* Ensures images fill their grid area without distortion */
        }
    </style>
</head>
<body>
    <h1>Sounding Canvas Project Report</h1>

<div class="section">
    <h2>1. Project Overview and Goals</h2>
    <p>
        The Sounding Canvas project integrates capacitive sensing technology with a Raspberry Pi 4 Model B for interactive sound generation. The core objective was to develop a system where touch or proximity to the canvas surface, detected by changes in capacitance, triggers corresponding audio output through connected speakers. This report details the hardware setup, software implementation, underlying electronics, and the progression from initial sensor testing to the final integrated artwork. 
    </p>
</div>

<div class="section">
    <h2>2. Hardware Architecture and Connections</h2>
    <h3>2.1 Components:</h3>
    <ul>
        <li>Arduino Duemilanove: Microcontroller responsible for reading capacitive sensor data and communicating with the Raspberry Pi via USB.</li>
        <li>Raspberry Pi 4 Model B with HiFiBerry Amp2 HAT: Single-board computer for audio processing, sound file playback, and amplification, powered through the HiFiBerry Amp2 HAT.</li>
        <li>Capacitive Sensors: Constructed from aluminum foil pads strategically placed on the back of the canvas.</li>
        <li>Resistors: 1.4 MΩ resistors (optimized through testing) used in each sensor circuit to control sensitivity.</li>
        <li>Loudspeakers: Two 4-inch speakers (20-40 W, 4 Ω impedance) for audio output, mounted to the wooden back panel.</li>
        <li>Breadboard and Wires: Used for prototyping and connecting initial circuits. Cabling within the final assembly is managed for cleanliness and safety.</li>
        <li>Wooden Back Panel: Provides structural support and mounting points for electronic components behind the canvas, with a 3 cm gap to accommodate them.</li>
    </ul>

    <h3>2.2 Sensor Connections:</h3>
    <p>
        Each of the four capacitive sensors utilizes a shared "send" pin (Arduino pin 2) and an individual "receive" pin (Arduino pins 4, 6, 8, and 10). A 1.4 MΩ resistor is connected between the send and the respective receive pin for each sensor.
    </p>
</div>

<div class="section">
    <h2>3. Capacitive Sensing Technology</h2>
    <h3>3.1 Circuit Explanation:</h3>
    <p>
        The capacitive sensing circuit operates by measuring the time delay in an RC (resistor-capacitor) network.
    </p>
    <ul>
        <li><strong>Send Pin:</strong> Outputs a pulsed signal to charge and discharge the capacitive sensor.</li>
        <li><strong>Receive Pin:</strong> Monitors the voltage changes resulting from the charging and discharging cycles.</li>
        <li><strong>Resistor (R):</strong> The 1.4 MΩ resistor controls the rate at which the capacitor charges. Optimization determined this value provided the best balance of sensitivity and stability.</li>
        <li><strong>Capacitor (C):</strong> The capacitance is influenced by the aluminum foil sensor, environmental factors, and, crucially, the proximity or touch of a user.</li>
    </ul>

    <h3>3.2 RC Delay and Sensitivity:</h3>
    <p>
        The RC time constant (<code>τ = R × C</code>) dictates the charging and discharging speed. Changes in capacitance (C) due to interaction alter this time constant, which is detected by the Arduino. Sensitivity was fine-tuned by adjusting the resistor value and the sampling rate within the <strong>CapacitiveSensor</strong> library. The aluminum foil sensor area of approximately <strong>40 square centimeters</strong> was found to offer reliable detection.
    </p>
</div>

<div class="section">
    <h2>4. Software Implementation</h2>
    <h3>4.1 Arduino Firmware:</h3>
    <p>
        The Arduino Duemilanove utilizes the <strong>CapacitiveSensor</strong> library by Paul Stoffregen to measure capacitance changes. The core initialization in the Arduino sketch is:
    </p>
    <pre>
CapacitiveSensor cs_sensorX = CapacitiveSensor(sendPin, receivePinX); // For each sensor
    </pre>
    <p>
        Sensitivity is managed through the number of samples taken by the library. Threshold values for sensor activation were determined experimentally via the Arduino IDE Serial Monitor to ensure accurate detection and minimize false triggers.
    </p>

    <h3>4.2 Raspberry Pi Software:</h3>
    <p>
        A Python program running on the Raspberry Pi 4 Model B is responsible for processing sensor data received from the Arduino and triggering audio playback. Key features include:
    </p>
    <ol>
        <li><strong>Serial Communication:</strong> Establishes communication with the Arduino over USB to receive sensor readings.</li>
        <li><strong>Threshold-Based Activation:</strong> Detects sensor activation when received values exceed predefined thresholds.</li>
        <li><strong>Debouncing:</strong> Implements a minimum two second delay between successive activations of the same sensor to prevent rapid re-triggering.</li>
        <li><strong>Randomized Sound Selection:</strong> For each of the four sensors, the program randomly selects one of four distinct sound samples from a corresponding folder. The sounds were created using sustained electric guitar recordings to provide sonic variety.</li>
        <li><strong>Parallel Playback:</strong> Allows for the simultaneous playback of sounds triggered by multiple sensor activations.</li>
    </ol>
    
    <h3>4.3 Arduino Interface & Event Manager</h3>
    <p>
        The Arduino Interface plays a crucial role in bridging the physical sensing hardware with the higher-level
        software logic running on the Raspberry Pi. It collects raw capacitive touch data from the sensor electrodes
        mounted behind the canvas and transmits meaningful interaction events to the main system.
    </p>
    <p>
        The Arduino aggregates <strong>CapacitiveSensor</strong> data and periodically transmits it over a USB serial
        connection to the Raspberry Pi. A custom Python module on the Raspberry Pi acts as a listener and parser for this
        serial data stream. The module maintains a queue of recent readings and applies event detection logic to identify
        new touch events, sustained contact, or release events.
    </p>
    <p>
        To ensure responsiveness and avoid false triggers, the system employs a hysteresis mechanism and configurable
        thresholds. For example, a touch is only registered if the measured signal exceeds a given activation threshold
        for a sustained number of readings. This reduces noise and ensures reliable operation even in environments with
        fluctuating electromagnetic conditions.
    </p>
    <p>
        Once events are detected, they are forwarded to the <em>Event Manager</em>, a central coordination module
        responsible for triggering corresponding actions within the system. These actions include activating sound
        playback through the Audio Engine, updating visual feedback, or logging interaction data.
    </p>
    <p>
        Beyond simple event forwarding, the Event Manager also performs adaptive learning based on user interaction
        patterns. Earlier versions of the system employed adaptive <strong>Markov models</strong> to infer probabilities
        of sequential gestures and transitions over time. This probabilistic model enabled context-aware reactions,
        anticipating likely next events and tailoring the sonic or visual response accordingly.
    </p>
    <p>
        Current development explores the use of <strong>Recurrent Neural Networks (RNNs)</strong> as a more powerful
        approach for modeling and detecting complex, temporally extended interaction sequences. This opens the
        possibility of deeper temporal understanding of user behavior, enriching the perceptual and expressive potential
        of the artwork.
    </p>
    <p>
        The modular design of the Arduino Interface and Event Manager ensures a clear separation between low-level
        hardware communication and high-level behavioral logic. This makes the system maintainable and extensible, so new
        sensor types or behaviors can be added with minimal impact on the overall architecture.
    </p>
</div>

<div class="section">
    <h2>5. Mounting and Running</h2>
    <p>
        In "Echoes of a Line", four capacitive sensors, made from aluminum foil pads, are mounted on the back of the canvas using adhesive to ensure they remain flat and secure. A non-conductive layer is placed between the foil and the canvas to prevent unintended triggers due to external electrical interference. The sensors are connected to the Arduino as described in Section 2.2.
    </p>
    <p>
        All electronic components, including the Arduino, Raspberry Pi with HiFiBerry Amp2, and loudspeakers, are securely mounted to a wooden back panel positioned behind the canvas. The loudspeakers are fixed using screws within circular recesses for stability. Cabling is carefully managed to ensure a clean and safe internal layout, with easy access for potential maintenance.
    </p>
    <p>
        Initial testing with a single sensor confirmed the successful detection of capacitance changes with fast and consistent readings. Subsequent testing with four sensors demonstrated the system's ability to detect individual sensor activations without interference. Threshold calibration ensured reliable responsiveness and minimized false positives.
    </p>
</div>
<iframe width="560" height="315" src="https://www.youtube.com/embed/D-_uQ7EWdfk?si=tJvAfXpfIp5dCwFd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
    <p>This interactive sound canvas represents the fusion of art and technology, offering a dynamic platform for creative expression. It exemplifies the potential of collaborative efforts between humans and AI in the realization of artistic and technical visions.</p>
    <div class="section">
    <h2>6. Communication Between Canvases</h2>
    <p>
        A unique and powerful aspect of the Sounding Canvas series is its ability to communicate across multiple artworks, creating a networked, collaborative experience. This inter-canvas communication is facilitated by a central <strong>Python server</strong> that acts as a hub for all connected Sounding Canvases.
    </p>

    <h3>6.1 WebSocket-Based Connectivity</h3>
    <p>
        Each individual Sounding Canvas establishes a persistent connection to the Python server using <strong>WebSockets</strong>. WebSockets provide a full-duplex communication channel over a single TCP connection, allowing for real-time, low-latency data exchange between the canvases and the server. This is crucial for the responsive and synchronous interaction desired across the networked artworks.
    </p>

    <h3>6.2 Routing Touch Events</h3>
    <p>
        When a user interacts with a specific Sounding Canvas (e.g., touches a sensor), the following communication flow occurs:
    </p>
    <ol>
        <li><strong>Event Detection:</strong> The touched canvas detects the sensor activation and processes it locally (as described in the "Software Implementation" section).</li>
        <li><strong>Event Transmission to Server:</strong> The canvas immediately sends a message to the central Python server via its established WebSocket connection. This message contains information about the touch event, such as the canvas ID and the activated sensor.</li>
        <li><strong>Server Routing:</strong> Upon receiving a touch event from any canvas, the Python server acts as a router. It takes the incoming event and <strong>broadcasts it to all other currently connected Sounding Canvases</strong>. This ensures that every canvas in the network is aware of interactions happening on any other canvas.</li>
        <li><strong>Reception and Response:</strong> Each receiving canvas processes the incoming event from the server. This allows for synchronized visual or auditory responses across the entire network of artworks, enabling complex, multi-canvas compositions or shared interactive experiences.</li>
    </ol>
    <p>
        This server-client architecture, powered by WebSockets, allows the Sounding Canvas series to transcend individual artworks, transforming them into a cohesive, interactive network where a touch on one canvas can resonate and influence the sonic landscape of all others.
    </p>
</div>

        <h4>Communication Logs:</h4>
        <p>Below are terminal logs illustrating the communication flow between the canvases and the central server.</p>
        <figure>
            <img src="../images/canvas-rome-log.png" 
                 alt="Terminal log of the Sounding Canvas in Rome, showing its connection as the first client and received touch events."
                 loading="lazy">
            <figcaption>Figure 5.1: Terminal Log - Canvas in Rome (Client 1)</figcaption>
        </figure>
        <figure>
            <img src="../images/canvas-barcelona-log.png" 
                 alt="Terminal log of the Sounding Canvas in Barcelona, showing its connection as the second client and received touch events."
                 loading="lazy">
            <figcaption>Figure 5.2: Terminal Log - Canvas in Barcelona (Client 2)</figcaption>
        </figure>
        <figure>
            <img src="../images/server-log.png" 
                 alt="Terminal log of the central Python server, showing incoming client connections and the routing of touch events to all connected canvases."
                 loading="lazy">
            <figcaption>Figure 5.3: Terminal Log - Central Python Server</figcaption>
        </figure>
    <h2>7. Distributed Minds</h2>

<p>
  The <em>Event Manager</em> doesn’t just trigger a sound; it makes a decision.
  In our RNN-based setup, the canvas maintains an internal goal and selects
  the next sound to maximize the probability of a desired future touch. This is
  why we call it a <em>Mind</em>,it reasons over history and projects into what comes next.
</p>

<p>
  By contrast, the Markov-based variant adapts transition probabilities on the fly,
  constantly rebalancing its expectations as interactions unfold. It chooses sounds
  according to these evolving adjustments, yielding a system that is responsive,
  lightweight, and context-aware.
</p>

<p>
  Personalization is built in. Owners may contribute additional
  <em>vectorialized information</em>,from preferences to session context,so the RNN can
  shape behavior to a specific person or place. Over time, each canvas can embody a
  distinct identity while preserving core interaction principles.
</p>

<p>
  The Event Manager also acts as a <strong>hub</strong> for inter-canvas communication.
  It can forward and receive events across the network, allowing multiple works to
  synchronize and influence one another. A gesture on one canvas can ripple across
  others,across a room or across the globe,so interaction is no longer confined to a
  single body or object, but becomes a shared, distributed experience.
</p>

<p>
  In this networked ecology, the canvases’ <em>Minds</em> are not only local
  interlocutors,responding to the person in front of them,but also
  <strong>mediators</strong> between distant participants. They translate local touch
  into distributed resonance, weaving separate encounters into a collective
  conversation of gestures and sounds.
</p>

    <p>  </p>
    <p><a href="https://luciamarock.github.io/Projects/SoundingCanvas.html"">BACK to the SoundingCanvas main page</a></p>
</body>
</html>

